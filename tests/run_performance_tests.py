#!/usr/bin/env python3
"""
Simple Performance Test Runner
Tests critical performance metrics for the bot
"""
import time
import sys
import os
import queue
import threading
import gc
import resource

# Add project root to Python path
sys.path.insert(0, os.path.join(os.path.dirname(os.path.abspath(__file__)), '..'))

# Import modules after path setup
from config import load_config, load_watch_config, build_monitored_sources
from bot.filters.keyword import check_whitelist, check_blacklist
from bot.filters.regex import check_whitelist_regex, check_blacklist_regex
from bot.filters.extract import extract_content
from bot.utils.dedup import (
    mark_message_processed, 
    is_message_processed, 
    register_processed_media_group, 
    is_media_group_processed
)
from bot.utils.status import set_user_state, get_user_state, update_user_state, clear_user_state
from bot.workers.message_worker import Message, MessageWorker
from constants import *


def get_memory_mb():
    """Get current memory usage in MB"""
    try:
        return resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1024
    except:
        return 0


def measure_time(func, *args, **kwargs):
    """Measure function execution time"""
    start = time.perf_counter()
    result = func(*args, **kwargs)
    end = time.perf_counter()
    return result, (end - start) * 1000  # ms


def run_benchmark(name, func, iterations=1000, warmup=10):
    """Run performance benchmark"""
    print(f"\n{'='*60}")
    print(f"ğŸ”¥ {name}")
    print(f"{'='*60}")
    
    # Warmup
    for _ in range(warmup):
        try:
            func()
        except:
            pass
    
    # Benchmark
    times = []
    for _ in range(iterations):
        try:
            _, exec_time = measure_time(func)
            times.append(exec_time)
        except Exception as e:
            print(f"Error: {e}")
            continue
    
    if not times:
        print("âŒ All iterations failed")
        return None
    
    # Statistics
    avg_time = sum(times) / len(times)
    min_time = min(times)
    max_time = max(times)
    
    print(f"è¿­ä»£æ¬¡æ•°: {iterations}")
    print(f"å¹³å‡æ—¶é—´: {avg_time:.4f} ms")
    print(f"æœ€å°æ—¶é—´: {min_time:.4f} ms")
    print(f"æœ€å¤§æ—¶é—´: {max_time:.4f} ms")
    print(f"ååé‡:   {1000/avg_time:.2f} ops/sec")
    
    return {
        'name': name,
        'iterations': iterations,
        'avg_ms': avg_time,
        'min_ms': min_time,
        'max_ms': max_time,
        'throughput': 1000/avg_time
    }


def test_filter_performance():
    """Test filter performance"""
    test_text = "Hello world, this is a test message with price 100 USD and email test@example.com"
    whitelist = ["hello", "test", "message"]
    blacklist = ["spam", "bad", "forbidden"]
    whitelist_regex = [r"\d+\s+USD", r"test"]
    blacklist_regex = [r"spam", r"forbidden"]
    extract_patterns = [r"\d+", r"\w+@\w+\.\w+"]
    
    results = []
    
    result = run_benchmark(
        "å…³é”®è¯ç™½åå•è¿‡æ»¤ (Keyword Whitelist)",
        lambda: check_whitelist(test_text, whitelist),
        iterations=10000
    )
    if result:
        results.append(result)
    
    result = run_benchmark(
        "å…³é”®è¯é»‘åå•è¿‡æ»¤ (Keyword Blacklist)",
        lambda: check_blacklist(test_text, blacklist),
        iterations=10000
    )
    if result:
        results.append(result)
    
    result = run_benchmark(
        "æ­£åˆ™ç™½åå•è¿‡æ»¤ (Regex Whitelist)",
        lambda: check_whitelist_regex(test_text, whitelist_regex),
        iterations=5000
    )
    if result:
        results.append(result)
    
    result = run_benchmark(
        "æ­£åˆ™é»‘åå•è¿‡æ»¤ (Regex Blacklist)",
        lambda: check_blacklist_regex(test_text, blacklist_regex),
        iterations=5000
    )
    if result:
        results.append(result)
    
    result = run_benchmark(
        "å†…å®¹æå– (Content Extraction)",
        lambda: extract_content(test_text, extract_patterns),
        iterations=5000
    )
    if result:
        results.append(result)
    
    return results


def test_deduplication_performance():
    """Test deduplication performance"""
    results = []
    
    result = run_benchmark(
        "æ ‡è®°æ¶ˆæ¯å·²å¤„ç† (Mark Message Processed)",
        lambda: mark_message_processed(123456, -100123456789),
        iterations=10000
    )
    if result:
        results.append(result)
    
    # Prepare for lookup test
    for i in range(1000):
        mark_message_processed(i, -100123456789)
    
    result = run_benchmark(
        "æ£€æŸ¥æ¶ˆæ¯æ˜¯å¦å·²å¤„ç† (Check Message Processed)",
        lambda: is_message_processed(500, -100123456789),
        iterations=10000
    )
    if result:
        results.append(result)
    
    result = run_benchmark(
        "æ³¨å†Œåª’ä½“ç»„ (Register Media Group)",
        lambda: register_processed_media_group(f"user_key_dest_forward_{time.time()}"),
        iterations=5000
    )
    if result:
        results.append(result)
    
    return results


def test_config_performance():
    """Test config management performance"""
    results = []
    
    result = run_benchmark(
        "åŠ è½½ä¸»é…ç½® (Load Config)",
        lambda: load_config(),
        iterations=1000
    )
    if result:
        results.append(result)
    
    result = run_benchmark(
        "åŠ è½½ç›‘æ§é…ç½® (Load Watch Config)",
        lambda: load_watch_config(),
        iterations=1000
    )
    if result:
        results.append(result)
    
    result = run_benchmark(
        "æ„å»ºç›‘æ§æºé›†åˆ (Build Monitored Sources)",
        lambda: build_monitored_sources(),
        iterations=1000
    )
    if result:
        results.append(result)
    
    return results


def test_state_management_performance():
    """Test state management performance"""
    results = []
    
    result = run_benchmark(
        "è®¾ç½®ç”¨æˆ·çŠ¶æ€ (Set User State)",
        lambda: set_user_state("user123", {"step": "test", "data": "value"}),
        iterations=10000
    )
    if result:
        results.append(result)
    
    # Prepare data for get test
    set_user_state("user123", {"step": "test"})
    
    result = run_benchmark(
        "è·å–ç”¨æˆ·çŠ¶æ€ (Get User State)",
        lambda: get_user_state("user123"),
        iterations=10000
    )
    if result:
        results.append(result)
    
    result = run_benchmark(
        "æ›´æ–°ç”¨æˆ·çŠ¶æ€ (Update User State)",
        lambda: update_user_state("user123", new_field="value"),
        iterations=10000
    )
    if result:
        results.append(result)
    
    return results


def test_queue_performance():
    """Test queue performance"""
    print("\n" + "="*60)
    print("ğŸ“¬ é˜Ÿåˆ—æ€§èƒ½æµ‹è¯•")
    print("="*60)
    
    q = queue.Queue()
    
    # Test enqueue
    _, enqueue_time = measure_time(lambda: [q.put(i) for i in range(1000)])
    print(f"å…¥é˜Ÿ1000æ¡æ¶ˆæ¯: {enqueue_time:.4f} ms ({1000000/enqueue_time:.0f} ops/sec)")
    
    # Test dequeue
    _, dequeue_time = measure_time(lambda: [q.get() for _ in range(1000)])
    print(f"å‡ºé˜Ÿ1000æ¡æ¶ˆæ¯: {dequeue_time:.4f} ms ({1000000/dequeue_time:.0f} ops/sec)")
    
    return {
        'enqueue_ms': enqueue_time,
        'dequeue_ms': dequeue_time,
        'enqueue_ops': 1000000/enqueue_time,
        'dequeue_ops': 1000000/dequeue_time
    }


def test_constants_access():
    """Test constants access performance"""
    print("\n" + "="*60)
    print("ğŸ”§ å¸¸é‡è®¿é—®æ€§èƒ½æµ‹è¯•")
    print("="*60)
    
    results = []
    
    # Test constant access
    result = run_benchmark(
        "å¸¸é‡è®¿é—® (Constant Access)",
        lambda: (MAX_RETRIES, MAX_FLOOD_RETRIES, OPERATION_TIMEOUT),
        iterations=100000
    )
    if result:
        results.append(result)
    
    # Test backoff calculation
    result = run_benchmark(
        "é€€é¿è®¡ç®— (Backoff Calculation)",
        lambda: get_backoff_time(2),
        iterations=100000
    )
    if result:
        results.append(result)
    
    return results


def generate_report(results):
    """Generate performance test report"""
    print("\n" + "="*60)
    print("ğŸ“Š æ€§èƒ½æµ‹è¯•æ€»ç»“æŠ¥å‘Š")
    print("="*60)
    
    print("\nğŸ¯ å…³é”®æ€§èƒ½æŒ‡æ ‡ (Key Performance Indicators)")
    print("-" * 60)
    
    if 'filters' in results and results['filters']:
        print("\nğŸ” è¿‡æ»¤å™¨æ€§èƒ½:")
        for r in results['filters']:
            print(f"  {r['name']:40s} {r['avg_ms']:8.4f} ms/op")
    
    if 'dedup' in results and results['dedup']:
        print("\nğŸ”„ å»é‡æ€§èƒ½:")
        for r in results['dedup']:
            print(f"  {r['name']:40s} {r['avg_ms']:8.4f} ms/op")
    
    if 'config' in results and results['config']:
        print("\nâš™ï¸  é…ç½®ç®¡ç†æ€§èƒ½:")
        for r in results['config']:
            print(f"  {r['name']:40s} {r['avg_ms']:8.4f} ms/op")
    
    if 'state' in results and results['state']:
        print("\nğŸ“ çŠ¶æ€ç®¡ç†æ€§èƒ½:")
        for r in results['state']:
            print(f"  {r['name']:40s} {r['avg_ms']:8.4f} ms/op")
    
    if 'queue' in results:
        print(f"\nğŸ“¬ é˜Ÿåˆ—æ€§èƒ½:")
        print(f"  å…¥é˜Ÿååé‡: {results['queue']['enqueue_ops']:.0f} ops/sec")
        print(f"  å‡ºé˜Ÿååé‡: {results['queue']['dequeue_ops']:.0f} ops/sec")
    
    if 'constants' in results and results['constants']:
        print(f"\nğŸ”§ å¸¸é‡è®¿é—®æ€§èƒ½:")
        for r in results['constants']:
            print(f"  {r['name']:40s} {r['avg_ms']:8.6f} ms/op ({r['throughput']:.0f} ops/sec)")
    
    print("\n" + "="*60)
    print("âœ… æ€§èƒ½æµ‹è¯•å®Œæˆï¼")
    print("="*60)


def main():
    """Run all performance tests"""
    print("\n" + "="*60)
    print("ğŸš€ Save-Restricted-Bot æ€§èƒ½æµ‹è¯•å¥—ä»¶")
    print("   Performance Testing Suite")
    print("="*60)
    
    results = {}
    
    try:
        # Filter performance
        print("\n[1/6] æµ‹è¯•è¿‡æ»¤å™¨æ€§èƒ½...")
        results['filters'] = test_filter_performance()
        
        # Deduplication performance
        print("\n[2/6] æµ‹è¯•å»é‡æ€§èƒ½...")
        results['dedup'] = test_deduplication_performance()
        
        # Config performance
        print("\n[3/6] æµ‹è¯•é…ç½®ç®¡ç†æ€§èƒ½...")
        results['config'] = test_config_performance()
        
        # State management performance
        print("\n[4/6] æµ‹è¯•çŠ¶æ€ç®¡ç†æ€§èƒ½...")
        results['state'] = test_state_management_performance()
        
        # Queue performance
        print("\n[5/6] æµ‹è¯•é˜Ÿåˆ—æ€§èƒ½...")
        results['queue'] = test_queue_performance()
        
        # Constants access performance
        print("\n[6/6] æµ‹è¯•å¸¸é‡è®¿é—®æ€§èƒ½...")
        results['constants'] = test_constants_access()
        
        # Generate report
        generate_report(results)
        
        return 0
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
