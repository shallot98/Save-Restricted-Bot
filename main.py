"""
Save-Restricted-Bot - Telegram Bot for Saving Restricted Content
Main entry point - coordinates all modules
"""
import pyrogram
from pyrogram import Client, filters
from pyrogram.errors import UserAlreadyParticipant, InviteHashExpired, UsernameNotOccupied, ChannelPrivate, UsernameInvalid, FloodWait
from pyrogram.types import InlineKeyboardMarkup, InlineKeyboardButton, CallbackQuery

import threading
import queue
import logging

# Initialize logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

# Import configuration management
from config import (
    load_config, getenv, load_watch_config, save_watch_config,
    build_monitored_sources, reload_monitored_sources, get_monitored_sources,
    DATA_DIR, CONFIG_DIR, MEDIA_DIR
)

# Import database
from database import init_database

# Import bot utilities
from bot.utils import (
    is_message_processed, mark_message_processed, cleanup_old_messages,
    user_states, cached_dest_peers
)
from bot.utils.peer import cache_peer, is_dest_cached, mark_dest_cached, mark_peer_failed, get_failed_peers
from bot.utils.progress import progress, downstatus, upstatus

# Import workers
from bot.workers import MessageWorker, Message

# Import handlers
from bot.handlers import set_bot_instance, set_acc_instance
from bot.handlers.commands import register_command_handlers, show_watch_menu

# Load main configuration
DATA = load_config()

# Get configuration values
bot_token = getenv("TOKEN", DATA)
api_hash = getenv("HASH", DATA)
api_id = getenv("ID", DATA)

# Create bot client
bot = Client("mybot", api_id=api_id, api_hash=api_hash, bot_token=bot_token)

# Create user client if session string available
ss = getenv("STRING", DATA)
if ss is not None:
    if DATA.get("STRING"):
        logger.info("âœ… ä½¿ç”¨ config.json ä¸­çš„ session string")
    else:
        logger.info("âœ… ä½¿ç”¨ç¯å¢ƒå˜é‡ STRING ä¸­çš„ session string")

    # å…ˆå°è¯•ä½¿ç”¨å·²æœ‰çš„ session æ–‡ä»¶ï¼ˆåŒ…å« Peer ç¼“å­˜ï¼‰
    import os
    os.makedirs("sessions", exist_ok=True)
    session_file = "sessions/myacc"
    if os.path.exists(f"{session_file}.session"):
        logger.info("ğŸ“‚ å‘ç°å·²æœ‰ Session æ–‡ä»¶ï¼Œå°†ä¿ç•™ Peer ç¼“å­˜")
        acc = Client(session_file, api_id=api_id, api_hash=api_hash)
    else:
        logger.info("ğŸ“ é¦–æ¬¡å¯åŠ¨ï¼Œä½¿ç”¨ Session String åˆ›å»º Session æ–‡ä»¶")
        acc = Client(session_file, api_id=api_id, api_hash=api_hash, session_string=ss)

    acc.start()
else:
    logger.warning("âš ï¸ æœªæ‰¾åˆ° session stringï¼Œacc å®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
    acc = None

# Set handler instances for use by modules
set_bot_instance(bot)
set_acc_instance(acc)

# Initialize message queue and worker thread
from constants import MAX_RETRIES

message_queue = queue.Queue()
message_worker = None
worker_thread = None

if acc is not None:
    message_worker = MessageWorker(message_queue, acc, max_retries=MAX_RETRIES)
    worker_thread = threading.Thread(target=message_worker.run, daemon=True, name="MessageWorker")
    worker_thread.start()
    logger.info("âœ… æ¶ˆæ¯é˜Ÿåˆ—å’Œå·¥ä½œçº¿ç¨‹å·²åˆå§‹åŒ–")

# Register command handlers
register_command_handlers(bot, acc)

# Import handlers from new modular structure
from bot.handlers.callbacks import callback_handler
from bot.handlers.messages import save, handle_private
from bot.handlers.watch_setup import (
    show_filter_options, show_filter_options_single,
    show_preserve_source_options, show_forward_mode_options,
    complete_watch_setup, complete_watch_setup_single,
    handle_add_source, handle_add_dest
)
from bot.utils.helpers import get_message_type
from constants import USAGE

# Register callback handler
@bot.on_callback_query()
def handle_callback(client, callback_query):
    callback_handler(client, callback_query)

# Register message handler for private messages
@bot.on_message(filters.text & filters.private & ~filters.command(["start", "help", "watch"]))
def handle_save(client, message):
    save(client, message)

# Register auto-forward handler (monitor)
if acc is not None:
    @acc.on_message((filters.channel | filters.group | filters.private) & (filters.incoming | filters.outgoing))
    def auto_forward(client: pyrogram.client.Client, message: pyrogram.types.messages_and_media.message.Message):
        """å¤„ç†é¢‘é“/ç¾¤ç»„/ç§èŠæ¶ˆæ¯ï¼ŒåŒ…æ‹¬è½¬å‘çš„æ¶ˆæ¯"""
        try:
            # Validate message object and its attributes
            if not message or not hasattr(message, 'chat') or not message.chat:
                logger.debug("è·³è¿‡ï¼šæ¶ˆæ¯å¯¹è±¡æ— æ•ˆæˆ–ç¼ºå°‘ chat å±æ€§")
                return
            
            # Validate chat ID
            if not hasattr(message.chat, 'id') or message.chat.id is None:
                logger.debug("è·³è¿‡ï¼šæ¶ˆæ¯ç¼ºå°‘æœ‰æ•ˆçš„ chat ID")
                return
            
            # Check for duplicate messages
            if not hasattr(message, 'id') or message.id is None:
                logger.debug("è·³è¿‡ï¼šæ¶ˆæ¯ç¼ºå°‘æœ‰æ•ˆçš„ message ID")
                return
            
            if is_message_processed(message.id, message.chat.id):
                logger.debug(f"â­ï¸ è·³è¿‡å·²å¤„ç†çš„æ¶ˆæ¯: chat_id={message.chat.id}, message_id={message.id}")
                return
            
            # Mark message as processed immediately to prevent duplicate processing
            mark_message_processed(message.id, message.chat.id)
            
            # Periodically clean up old message records
            from bot.utils.dedup import processed_messages
            from constants import MESSAGE_CACHE_CLEANUP_THRESHOLD
            if len(processed_messages) > MESSAGE_CACHE_CLEANUP_THRESHOLD:
                cleanup_old_messages()
            
            # Log message type
            if message.outgoing:
                logger.debug(f"ğŸ“¤ outgoingæ¶ˆæ¯ï¼ˆç”±Botè½¬å‘ï¼‰: chat_id={message.chat.id}, message_id={message.id}")
            else:
                logger.debug(f"ğŸ“¥ incomingæ¶ˆæ¯ï¼ˆå¤–éƒ¨æ¥æºï¼‰: chat_id={message.chat.id}, message_id={message.id}")
            
            # Get source chat ID
            source_chat_id = str(message.chat.id)
            
            # Early filter: check if this source is monitored
            monitored_sources = get_monitored_sources()
            if source_chat_id not in monitored_sources:
                return
            
            logger.info(f"ğŸ”” ç›‘æ§æºæ¶ˆæ¯: chat_id={source_chat_id}, message_id={message.id}")
            
            # Cache source peer to avoid "Peer id invalid" errors
            if not is_dest_cached(source_chat_id):
                logger.info(f"ğŸ”„ æºé¢‘é“æœªç¼“å­˜ï¼Œå°è¯•å»¶è¿ŸåŠ è½½: {source_chat_id}")
                success = cache_peer(acc, source_chat_id, "æºé¢‘é“")
                if not success:
                    logger.warning(f"âš ï¸ å»¶è¿ŸåŠ è½½æºé¢‘é“å¤±è´¥ï¼Œç»§ç»­å¤„ç†ï¼ˆè®°å½•æ¨¡å¼ä¸å—å½±å“ï¼‰")
                else:
                    logger.info(f"âœ… å»¶è¿ŸåŠ è½½æºé¢‘é“æˆåŠŸ: {source_chat_id}")
            
            # Get message text
            message_text = message.text or message.caption or ""
            
            # Find all matching watch configs
            watch_config = load_watch_config()
            enqueued_count = 0
            
            for user_id, watches in watch_config.items():
                for watch_key, watch_data in watches.items():
                    if isinstance(watch_data, dict):
                        watch_source = str(watch_data.get("source", ""))
                        dest = watch_data.get("dest")
                        record_mode = watch_data.get("record_mode", False)
                        
                        # Match source
                        if watch_source != source_chat_id:
                            continue
                        
                        logger.info(f"âœ… åŒ¹é…åˆ°ç›‘æ§ä»»åŠ¡: user={user_id}, source={source_chat_id}")
                        
                        # Cache destination peer if in forward mode
                        dest_chat_id = dest if not record_mode else None
                        dest_peer_ready = True  # Assume ready for record mode
                        
                        if dest_chat_id and dest_chat_id != "me":
                            # Forward mode - must have destination peer cached
                            if not is_dest_cached(dest_chat_id):
                                logger.info(f"ğŸ”„ ç›®æ ‡é¢‘é“æœªç¼“å­˜ï¼Œå°è¯•å»¶è¿ŸåŠ è½½: {dest_chat_id}")
                                success = cache_peer(acc, dest_chat_id, "ç›®æ ‡é¢‘é“")
                                if success:
                                    logger.info(f"âœ… å»¶è¿ŸåŠ è½½ç›®æ ‡é¢‘é“æˆåŠŸ: {dest_chat_id}")
                                    dest_peer_ready = True
                                else:
                                    logger.error(f"âŒ å»¶è¿ŸåŠ è½½ç›®æ ‡é¢‘é“å¤±è´¥: {dest_chat_id}")
                                    logger.error(f"   æ¶ˆæ¯å°†è¢«è·³è¿‡ï¼Œç­‰å¾…ä¸‹æ¬¡é‡è¯•ï¼ˆ60ç§’åï¼‰")
                                    dest_peer_ready = False
                            else:
                                logger.debug(f"âœ“ ç›®æ ‡é¢‘é“å·²ç¼“å­˜: {dest_chat_id}")
                        
                        # Skip enqueuing if destination peer is not ready for forward mode
                        if not dest_peer_ready:
                            logger.warning(f"â­ï¸ è·³è¿‡æ¶ˆæ¯ï¼ˆç›®æ ‡é¢‘é“æœªå°±ç»ªï¼‰: user={user_id}, dest={dest_chat_id}")
                            continue
                        
                        # Media group deduplication
                        from bot.utils.dedup import is_media_group_processed, register_processed_media_group
                        
                        if message.media_group_id:
                            mode_suffix = "record" if record_mode else "forward"
                            media_group_key = f"{user_id}_{watch_key}_{dest_chat_id}_{mode_suffix}_{message.media_group_id}"
                            
                            if is_media_group_processed(media_group_key):
                                logger.debug(f"â­ï¸ è·³è¿‡å·²å¤„ç†çš„åª’ä½“ç»„: {media_group_key}")
                                continue
                            
                            # Register as processed
                            register_processed_media_group(media_group_key)
                            logger.info(f"ğŸ“¸ é¦–æ¬¡å¤„ç†åª’ä½“ç»„: {media_group_key}")
                        
                        # Create message object
                        msg_obj = Message(
                            user_id=user_id,
                            watch_key=watch_key,
                            message=message,
                            watch_data=watch_data,
                            source_chat_id=source_chat_id,
                            dest_chat_id=dest_chat_id,
                            message_text=message_text,
                            media_group_key=f"{user_id}_{watch_key}_{message.media_group_id}" if message.media_group_id else None
                        )
                        
                        # Enqueue message for processing
                        message_queue.put(msg_obj)
                        enqueued_count += 1
                        logger.info(f"ğŸ“¬ æ¶ˆæ¯å·²å…¥é˜Ÿ: user={user_id}, source={source_chat_id}, é˜Ÿåˆ—å¤§å°={message_queue.qsize()}")
            
            if enqueued_count > 0:
                logger.info(f"âœ… æœ¬æ¬¡å…±å…¥é˜Ÿ {enqueued_count} æ¡æ¶ˆæ¯")
        
        except (ValueError, KeyError) as e:
            error_msg = str(e)
            if "Peer id invalid" not in error_msg and "ID not found" not in error_msg:
                logger.error(f"âš ï¸ auto_forward é”™è¯¯: {type(e).__name__}: {e}", exc_info=True)
        except Exception as e:
            logger.error(f"âš ï¸ auto_forward æ„å¤–é”™è¯¯: {type(e).__name__}: {e}", exc_info=True)


def _collect_source_ids(watch_config):
    """Collect source channel IDs that need to be cached"""
    source_ids = set()
    
    for user_id, watches in watch_config.items():
        for watch_key, watch_data in watches.items():
            if isinstance(watch_data, dict):
                source_id = watch_data.get("source", watch_key.split("|")[0] if "|" in watch_key else watch_key)
            else:
                source_id = watch_key
            
            # Add valid channel IDs (negative IDs, excluding special values)
            if source_id and source_id not in ["æœªçŸ¥æ¥æº", "me"]:
                try:
                    if int(source_id) < 0:
                        source_ids.add(source_id)
                except (ValueError, TypeError):
                    pass
    
    return source_ids


def _collect_dest_ids(watch_config):
    """Collect destination channel IDs that need to be cached"""
    dest_ids = set()
    
    for user_id, watches in watch_config.items():
        for watch_key, watch_data in watches.items():
            if isinstance(watch_data, dict):
                dest_id = watch_data.get("dest")
                record_mode = watch_data.get("record_mode", False)
                
                # Only cache forward mode destinations
                if not record_mode and dest_id and dest_id != "me":
                    try:
                        int(dest_id)  # Validate it's a numeric ID
                        dest_ids.add(dest_id)
                    except (ValueError, TypeError):
                        pass
    
    return dest_ids


def _cache_channels(acc, channel_ids, channel_type="é¢‘é“"):
    """Cache channel IDs to avoid Peer ID errors
    
    Args:
        acc: User client instance
        channel_ids: Set of channel IDs to cache
        channel_type: Type description for logging
        
    Returns:
        Tuple of (cached_count, total_count)
    """
    if not channel_ids:
        return 0, 0
    
    print(f"ğŸ”„ é¢„åŠ è½½{channel_type}ä¿¡æ¯åˆ°ç¼“å­˜...")
    cached_count = 0
    
    for channel_id in channel_ids:
        try:
            acc.get_chat(int(channel_id))
            cached_count += 1
            print(f"   âœ… å·²ç¼“å­˜: {channel_id}")
        except Exception as e:
            print(f"   âš ï¸ æ— æ³•ç¼“å­˜ {channel_id}: {str(e)}")
    
    print(f"ğŸ“¦ æˆåŠŸç¼“å­˜ {cached_count}/{len(channel_ids)} ä¸ª{channel_type}\n")
    return cached_count, len(channel_ids)


def _cache_dest_peers(acc, dest_ids):
    """Cache destination peers with detailed information
    
    Args:
        acc: User client instance
        dest_ids: Set of destination IDs to cache
        
    Returns:
        Tuple of (cached_count, total_count, failed_list)
    """
    if not dest_ids:
        return 0, 0, []
    
    print("ğŸ”„ é¢„åŠ è½½ç›®æ ‡Peerä¿¡æ¯åˆ°ç¼“å­˜...")
    cached_count = 0
    failed_dests = []
    
    for dest_id in dest_ids:
        try:
            dest_chat = acc.get_chat(int(dest_id))
            cached_count += 1
            
            # Extract chat name
            if hasattr(dest_chat, 'first_name') and dest_chat.first_name:
                chat_name = dest_chat.first_name
            elif hasattr(dest_chat, 'title') and dest_chat.title:
                chat_name = dest_chat.title
            elif hasattr(dest_chat, 'username') and dest_chat.username:
                chat_name = dest_chat.username
            else:
                chat_name = "Unknown"
            
            is_bot = " ğŸ¤–" if hasattr(dest_chat, 'is_bot') and dest_chat.is_bot else ""
            print(f"   âœ… å·²ç¼“å­˜ç›®æ ‡: {dest_id} ({chat_name}{is_bot})")
            
            mark_dest_cached(dest_id)
        except FloodWait as e:
            print(f"   âš ï¸ é™æµ: ç›®æ ‡ {dest_id}ï¼Œç­‰å¾… {e.value} ç§’")
            failed_dests.append(dest_id)
            mark_peer_failed(dest_id)
        except Exception as e:
            print(f"   âš ï¸ æ— æ³•ç¼“å­˜ç›®æ ‡ {dest_id}: {str(e)}")
            failed_dests.append(dest_id)
            mark_peer_failed(dest_id)
    
    print(f"ğŸ“¦ æˆåŠŸç¼“å­˜ {cached_count}/{len(dest_ids)} ä¸ªç›®æ ‡Peer")
    
    if failed_dests:
        print(f"ğŸ’¡ ç¼“å­˜å¤±è´¥çš„ç›®æ ‡ï¼ˆå…±{len(failed_dests)}ä¸ªï¼‰: {', '.join(failed_dests)}")
        print(f"   è¿™äº›ç›®æ ‡å°†åœ¨æ¥æ”¶åˆ°ç¬¬ä¸€æ¡æ¶ˆæ¯æ—¶è‡ªåŠ¨é‡è¯•å»¶è¿ŸåŠ è½½\n")
    else:
        print()
    
    return cached_count, len(dest_ids), failed_dests


def initialize_peer_cache_on_startup_with_retry(acc, max_retries=3):
    """å¸¦é‡è¯•çš„Peerç¼“å­˜åˆå§‹åŒ–
    
    ç¡®ä¿accå®Œå…¨è¿æ¥åå†åˆå§‹åŒ–ç¼“å­˜ï¼Œå¦‚æœå¤±è´¥è‡ªåŠ¨é‡è¯•
    
    Args:
        acc: User client instance
        max_retries: Maximum number of retry attempts (default: 3)
        
    Returns:
        bool: True if all peers cached successfully, False otherwise
    """
    import time
    
    try:
        watch_config = load_watch_config()
        all_peers = set()
        
        # ç¬¬ä¸€æ­¥ï¼šæ”¶é›†æ‰€æœ‰æºå’Œç›®æ ‡é¢‘é“ID
        for user_id, watches in watch_config.items():
            for watch_key, watch_data in watches.items():
                if isinstance(watch_data, dict):
                    source_id = watch_data.get("source")
                    dest_id = watch_data.get("dest")
                    
                    if source_id:
                        try:
                            all_peers.add(int(source_id))
                        except (ValueError, TypeError):
                            pass
                    
                    if dest_id and dest_id != "me":
                        try:
                            all_peers.add(int(dest_id))
                        except (ValueError, TypeError):
                            pass
        
        if not all_peers:
            logger.info("ğŸ“­ æ²¡æœ‰é…ç½®çš„Peeréœ€è¦åˆå§‹åŒ–")
            return True
        
        # å°è¯•åˆå§‹åŒ–ï¼Œæœ€å¤šé‡è¯• max_retries æ¬¡
        for attempt in range(max_retries):
            try:
                logger.info("="*60)
                logger.info(f"âš¡ ç¬¬ {attempt+1}/{max_retries} æ¬¡åˆå§‹åŒ– {len(all_peers)} ä¸ªPeerç¼“å­˜...")
                logger.info("="*60)
                
                success_count = 0
                failed_peers = []
                
                for peer_id in sorted(all_peers):
                    try:
                        # å…³é”®ï¼šget_chat() ä¼šåˆå§‹åŒ–Peerç¼“å­˜
                        chat = acc.get_chat(peer_id)
                        success_count += 1
                        
                        # Extract chat name
                        if hasattr(chat, 'title') and chat.title:
                            chat_name = chat.title
                        elif hasattr(chat, 'first_name') and chat.first_name:
                            chat_name = chat.first_name
                        elif hasattr(chat, 'username') and chat.username:
                            chat_name = f"@{chat.username}"
                        else:
                            chat_name = "Unknown"
                        
                        # Check if bot
                        is_bot = " ğŸ¤–" if hasattr(chat, 'is_bot') and chat.is_bot else ""
                        
                        logger.info(f"   âœ… {peer_id}: {chat_name}{is_bot}")
                        
                        # Mark as cached in our tracking system
                        mark_dest_cached(str(peer_id))
                        
                    except Exception as e:
                        error_msg = str(e)[:60]
                        failed_peers.append((peer_id, error_msg))
                        logger.warning(f"   âš ï¸ {peer_id}: {error_msg}")
                
                logger.info("="*60)
                logger.info(f"âœ… Peerç¼“å­˜åˆå§‹åŒ–å®Œæˆ: {success_count}/{len(all_peers)} æˆåŠŸ")
                
                # å¦‚æœå…¨éƒ¨æˆåŠŸï¼Œè¿”å›
                if success_count == len(all_peers):
                    logger.info("="*60)
                    logger.info("")
                    return True
                
                # å¦‚æœéƒ¨åˆ†å¤±è´¥ï¼Œæ˜¾ç¤ºè¯Šæ–­ä¿¡æ¯
                if failed_peers:
                    logger.warning(f"âš ï¸ å¤±è´¥çš„Peer (å…±{len(failed_peers)}ä¸ª):")
                    for peer_id, error in failed_peers:
                        logger.warning(f"   - {peer_id}: {error}")
                        mark_peer_failed(str(peer_id))
                
                # å¦‚æœè¿˜æœ‰é‡è¯•æœºä¼šï¼Œç­‰å¾…åé‡è¯•
                if attempt < max_retries - 1:
                    wait_time = (attempt + 1) * 2
                    logger.info(f"â³ ç­‰å¾… {wait_time} ç§’åé‡è¯•ï¼ˆè¿˜æœ‰ {max_retries - attempt - 1} æ¬¡æœºä¼šï¼‰...")
                    logger.info("="*60)
                    logger.info("")
                    time.sleep(wait_time)
                else:
                    logger.info(f"ğŸ’¡ å¤±è´¥çš„Peerå°†åœ¨æ¥æ”¶åˆ°ç¬¬ä¸€æ¡æ¶ˆæ¯æ—¶è‡ªåŠ¨é‡è¯•å»¶è¿ŸåŠ è½½")
                    logger.info("="*60)
                    logger.info("")
                
            except Exception as e:
                logger.error(f"âŒ åˆå§‹åŒ–å¼‚å¸¸: {e}", exc_info=True)
                
                if attempt < max_retries - 1:
                    wait_time = (attempt + 1) * 2
                    logger.info(f"â³ å¼‚å¸¸åç­‰å¾… {wait_time} ç§’å†è¯•...")
                    logger.info("="*60)
                    logger.info("")
                    time.sleep(wait_time)
        
        logger.warning(f"âš ï¸ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({max_retries})ï¼ŒPeerç¼“å­˜åˆå§‹åŒ–æœªèƒ½å®Œå…¨å®Œæˆ")
        logger.info("")
        return False
        
    except Exception as e:
        logger.error(f"âŒ Peerç¼“å­˜åˆå§‹åŒ–å¤±è´¥: {e}", exc_info=True)
        return False


def import_watch_config_on_startup(acc):
    """åœ¨å¯åŠ¨æ—¶å¯¼å…¥é…ç½®ï¼Œå¤ç”¨æ‰‹åŠ¨æ·»åŠ çš„é€»è¾‘
    
    è¯¥å‡½æ•°æ¨¡æ‹Ÿæ‰‹åŠ¨æ·»åŠ ç›‘æ§æ—¶çš„åˆå§‹åŒ–æµç¨‹ï¼Œç¡®ä¿ä½¿ç”¨ç›¸åŒçš„ä»£ç è·¯å¾„
    """
    import time
    
    logger.info("="*60)
    logger.info("ğŸ”„ å¼€å§‹å¯¼å…¥ç›‘æ§é…ç½®...")
    logger.info("="*60)
    
    try:
        watch_config = load_watch_config()
        
        if not watch_config:
            logger.info("ğŸ“­ æ²¡æœ‰ç›‘æ§é…ç½®éœ€è¦å¯¼å…¥")
            return True
        
        # ç»Ÿè®¡é…ç½®æ•°é‡
        total_configs = sum(len(watches) for watches in watch_config.values())
        logger.info(f"ğŸ“‹ æ‰¾åˆ° {total_configs} ä¸ªç›‘æ§é…ç½®")
        
        success_count = 0
        failed_count = 0
        
        for user_id, watches in watch_config.items():
            logger.info(f"\nğŸ‘¤ ç”¨æˆ· {user_id} çš„é…ç½®:")
            
            for watch_key, watch_data in watches.items():
                try:
                    # è§£æé…ç½®
                    if isinstance(watch_data, dict):
                        source_id = watch_data.get("source")
                        dest_id = watch_data.get("dest")
                        record_mode = watch_data.get("record_mode", False)
                    else:
                        # æ—§æ ¼å¼å…¼å®¹
                        source_id = watch_key
                        dest_id = watch_data
                        record_mode = False
                    
                    # è®°å½•é…ç½®ä¿¡æ¯ï¼ˆä¸å¼ºåˆ¶åˆå§‹åŒ–ï¼Œæ”¹ä¸ºå»¶è¿ŸåŠ è½½ï¼‰
                    if source_id and source_id != "me":
                        logger.info(f"   ğŸ“Œ æºé¢‘é“: {source_id} (å°†åœ¨æ”¶åˆ°æ¶ˆæ¯æ—¶è‡ªåŠ¨åˆå§‹åŒ–)")

                    if not record_mode and dest_id and dest_id != "me":
                        logger.info(f"   ğŸ“Œ ç›®æ ‡é¢‘é“: {dest_id} (å°†åœ¨è½¬å‘æ—¶è‡ªåŠ¨åˆå§‹åŒ–)")
                    elif record_mode:
                        logger.info(f"   ğŸ“ ç›®æ ‡: è®°å½•æ¨¡å¼")
                    
                    
                    success_count += 1
                    
                except Exception as e:
                    logger.error(f"   âŒ é…ç½®å¯¼å…¥å¤±è´¥ {watch_key}: {str(e)}")
                    failed_count += 1
                
                # é¿å…è§¦å‘é™æµï¼Œæ·»åŠ å°å»¶è¿Ÿ
                time.sleep(0.2)
        
        logger.info("")
        logger.info("="*60)
        logger.info(f"âœ… é…ç½®å¯¼å…¥å®Œæˆ: {success_count}/{total_configs} æˆåŠŸ")
        
        if failed_count > 0:
            logger.warning(f"âš ï¸ {failed_count} ä¸ªé…ç½®åˆå§‹åŒ–å¤±è´¥ï¼Œå°†åœ¨æ¥æ”¶æ¶ˆæ¯æ—¶è‡ªåŠ¨é‡è¯•")
        
        logger.info("="*60)
        logger.info("")
        
        return success_count > 0
        
    except Exception as e:
        logger.error(f"âŒ å¯¼å…¥é…ç½®æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return False


def _print_watch_tasks(watch_config):
    """Print configured watch tasks"""
    record_mode_count = sum(
        1 for watches in watch_config.values()
        for watch_data in watches.values()
        if isinstance(watch_data, dict) and watch_data.get("record_mode", False)
    )
    
    if record_mode_count > 0:
        print(f"ğŸ” é…ç½®çš„è®°å½•æ¨¡å¼ä»»åŠ¡: {record_mode_count} ä¸ª\n")
    
    for user_id, watches in watch_config.items():
        print(f"ğŸ‘¤ ç”¨æˆ· {user_id}:")
        for watch_key, watch_data in watches.items():
            if isinstance(watch_data, dict):
                source_id = watch_data.get("source", watch_key.split("|")[0] if "|" in watch_key else watch_key)
                dest_id = watch_data.get("dest", "æœªçŸ¥")
                record_mode = watch_data.get("record_mode", False)
                
                source_id = source_id or "æœªçŸ¥æ¥æº"
                dest_id = dest_id or "æœªçŸ¥ç›®æ ‡"
                
                if record_mode:
                    print(f"   ğŸ“ {source_id} â†’ è®°å½•æ¨¡å¼")
                else:
                    print(f"   ğŸ“¤ {source_id} â†’ {dest_id}")
            else:
                source_display = watch_key or "æœªçŸ¥æ¥æº"
                dest_display = watch_data or "æœªçŸ¥ç›®æ ‡"
                print(f"   ğŸ“¤ {source_display} â†’ {dest_display}")
        print()


def print_startup_config():
    """Print startup configuration"""
    # âš¡ å¯åŠ¨æ—¶å¼ºåˆ¶é‡æ–°åŠ è½½ç›‘æ§æºï¼Œç¡®ä¿ä½¿ç”¨æœ€æ–°é…ç½®
    reload_monitored_sources()
    
    monitored = get_monitored_sources()
    logger.info(f"ğŸ”„ å¯åŠ¨æ—¶å·²åŠ è½½ {len(monitored)} ä¸ªç›‘æ§æºé¢‘é“")
    
    print("\n" + "="*60)
    print("ğŸ¤– Telegram Save-Restricted Bot å¯åŠ¨æˆåŠŸ")
    print("="*60)
    
    if acc is not None:
        print("\nğŸ”§ æ¶ˆæ¯é˜Ÿåˆ—ç³»ç»Ÿå·²å¯ç”¨")
        print("   - æ¶ˆæ¯å¤„ç†æ¨¡å¼ï¼šé˜Ÿåˆ— + å·¥ä½œçº¿ç¨‹")
        from constants import MAX_RETRIES
        print(f"   - æœ€å¤§é‡è¯•æ¬¡æ•°ï¼š{MAX_RETRIES} æ¬¡")
        print("   - è‡ªåŠ¨æ•…éšœæ¢å¤ï¼šæ˜¯")
    
    watch_config = load_watch_config()
    if not watch_config:
        print("\nğŸ“‹ å½“å‰æ²¡æœ‰ç›‘æ§ä»»åŠ¡")
    else:
        total_tasks = sum(len(watches) for watches in watch_config.values())
        print(f"\nğŸ“‹ å·²åŠ è½½ {len(watch_config)} ä¸ªç”¨æˆ·çš„ {total_tasks} ä¸ªç›‘æ§ä»»åŠ¡ï¼š\n")
        
        # Print watch tasks
        _print_watch_tasks(watch_config)
        
        # å¯åŠ¨æ—¶è‡ªåŠ¨å¯¼å…¥é…ç½® - å¤ç”¨æ‰‹åŠ¨æ·»åŠ çš„é€»è¾‘
        if acc is not None:
            import time
            print("")  # ç©ºè¡Œåˆ†éš”
            logger.info("â³ ç­‰å¾…Sessionå®Œå…¨å»ºç«‹...")
            time.sleep(8)

            # ä½¿ç”¨ç®€åŒ–çš„å¯¼å…¥é€»è¾‘ï¼Œå¤ç”¨æ‰‹åŠ¨æ·»åŠ çš„ä»£ç è·¯å¾„
            import_watch_config_on_startup(acc)
    
    print("\n" + "="*60)
    print("âœ… æœºå™¨äººå·²å°±ç»ªï¼Œæ­£åœ¨ç›‘å¬æ¶ˆæ¯...")
    print("="*60 + "\n")


# Initialize database
print("\nğŸ”§ åˆå§‹åŒ–æ•°æ®åº“ç³»ç»Ÿ...")
try:
    init_database()
except Exception as e:
    print(f"âš ï¸ æ•°æ®åº“åˆå§‹åŒ–æ—¶å‘ç”Ÿé”™è¯¯: {e}")
    print("âš ï¸ ç»§ç»­å¯åŠ¨ï¼Œä½†è®°å½•æ¨¡å¼å¯èƒ½æ— æ³•å·¥ä½œ")

# Print startup configuration
print_startup_config()

# Start bot
bot.run()
if acc is not None:
    acc.stop()
